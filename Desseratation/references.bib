@article{Wu2020,
   abstract = {Computer vision has achieved remarkable success by (a) representing images as uniformly-arranged pixel arrays and (b) convolving highly-localized features. However, convolutions treat all image pixels equally regardless of importance; explicitly model all concepts across all images, regardless of content; and struggle to relate spatially-distant concepts. In this work, we challenge this paradigm by (a) representing images as semantic visual tokens and (b) running transformers to densely model token relationships. Critically, our Visual Transformer operates in a semantic token space, judiciously attending to different image parts based on context. This is in sharp contrast to pixel-space transformers that require orders-of-magnitude more compute. Using an advanced training recipe, our VTs significantly outperform their convolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to 7 points while using fewer FLOPs and parameters. For semantic segmentation on LIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points higher mIoU while reducing the FPN module's FLOPs by 6.5x.},
   author = {Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},
   month = {6},
   title = {Visual Transformers: Token-based Image Representation and Processing for Computer Vision},
   url = {http://arxiv.org/abs/2006.03677},
   year = {2020},
}
@article{Luo2020,
   abstract = {Both mobile edge cloud (MEC) and software-defined networking (SDN) are technologies for next generation mobile networks. In this paper, we propose to simultaneously optimize energy consumption and quality of experience (QoE) metrics in video streaming over software-defined mobile networks (SDMN) combined with MEC. Specifically, we propose a novel mechanism to jointly consider buffer dynamics, video quality adaption, edge caching, video transcoding and transmission. First, we assume that the time-varying channel is a discrete-time Markov chain (DTMC). Then, based on this assumption, we formulate two optimization problems which can be depicted as a constrained Markov decision process (CMDP) and a Markov decision process (MDP). Then, we transform the CMDP problem into regular MDP by deploying Lyapunov technique. We utilize asynchronous advantage actor-critic (A3C) algorithm, one of the model-free deep reinforcement learning (DRL) methods, to solve the corresponding MDP issues. Simulation results are presented to show that the proposed scheme can achieve the goal of energy saving and QoE enhancement with the corresponding constraints satisfied.},
   author = {Jia Luo and F. Richard Yu and Qianbin Chen and Lun Tang},
   doi = {10.1109/TWC.2019.2955129},
   issn = {15582248},
   issue = {3},
   journal = {IEEE Transactions on Wireless Communications},
   keywords = {Lyapunov technique,Software defined mobile networks,adaptive video streaming,deep reinforcement learning,mobile edge cloud},
   month = {3},
   pages = {1577-1592},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Adaptive video streaming with edge caching and video transcoding over software-defined mobile networks: A deep reinforcement learning approach},
   volume = {19},
   year = {2020},
}
@inproceedings{Zhang2020,
   abstract = {Machine learning models, particularly reinforcement learning (RL), have demonstrated great potential in optimizing video streaming applications. However, the state-of-the-art solutions are limited to an "offline learning" paradigm, i.e., the RL models are trained in simulators and then are operated in real networks. As a result, they inevitably suffer from the simulation-to-reality gap, showing far less satisfactory performance under real conditions compared with simulated environment. In this work, we close the gap by proposing OnRL, an online RL framework for real-time mobile video telephony. OnRL puts many individual RL agents directly into the video telephony system, which make video bitrate decisions in real-time and evolve their models over time. OnRL then aggregates these agents to form a high-level RL model that can help each individual to react to unseen network conditions. Moreover, OnRL incorporates novel mechanisms to handle the adverse impacts of inherent video traffic dynamics, and to eliminate risks of quality degradation caused by the RL model's exploration attempts. We implement OnRL on a mainstream operational video telephony system, Alibaba Taobao-live. In a month-long evaluation with 543 hours of video sessions from 151 real-world mobile users, OnRL out-performs the prior algorithms significantly, reducing video stalling rate by 14.22% while maintaining similar video quality. CCS CONCEPTS • Networks → Transport protocols; Mobile networks; • Computing methodologies → Machine learning.},
   author = {Huanhuan Zhang and Anfu Zhou and Jiamin Lu and Ruoxuan Ma and Yuhan Hu and Cong Li and Xinyu Zhang and Huadong Ma and Xiaojiang Chen},
   doi = {10.1145/3372224.3419186},
   isbn = {9781450370851},
   issue = {20},
   journal = {dl.acm.org},
   keywords = {Online Learning,Reinforcement Learning,Video Telephony},
   month = {9},
   pages = {1-14},
   publisher = {ACM},
   title = {OnRL},
   volume = {16},
   url = {https://dl.acm.org/doi/abs/10.1145/3372224.3419186?casa_token=CoB6Nzm-AAQAAAAA:_tWUDf6MB7gS3Dr5W36Vi_rccx9rRa3az1bLAiQvmL7kfyGQV9-6vXtClzYLG0rzLZnqihbFUHlZFg},
   year = {2020},
}
@article{,
   author = {Y Guo and FR Yu and J An and K Yang and C Yu - IEEE Transactions on … and undefined 2020},
   journal = {ieeexplore.ieee.org},
   title = {Adaptive bitrate streaming in wireless networks with transcoding at network edge using deep reinforcement learning},
   url = {https://ieeexplore.ieee.org/abstract/document/8964491/?casa_token=Hh6Ax-HI15EAAAAA:nPc4tbUfiXDO7RHHZyofBMDpiVPmkEGkZUYdYKF9Yj4biWQ5SLajlkl1i4XcF_-XZw58tOWqWU4},
}
@article{,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
   author = {Ashish Vaswani and Google Brain and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Łukasz Kaiser and Illia Polosukhin},
   journal = {proceedings.neurips.cc},
   title = {Attention is all you need},
   url = {https://proceedings.neurips.cc/paper/7181-attention-is-all-you-need},
}
@article{,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
   author = {Ashish Vaswani and Google Brain and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Łukasz Kaiser and Illia Polosukhin},
   title = {Attention Is All You Need},
}
@article{Hochreiter1997,
   abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back ow. We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called \Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through \constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
   author = {S Hochreiter and J Schmidhuber - Neural computation and undefined 1997},
   issue = {8},
   journal = {ieeexplore.ieee.org},
   pages = {1735-1780},
   title = {Long short-term memory},
   volume = {9},
   url = {https://ieeexplore.ieee.org/abstract/document/6795963/},
   year = {1997},
}
@article{Wang2019,
   abstract = {Dynamic Adaptive Streaming over HTTP (DASH) has been widely adopted to deal with such user diversity as network conditions and device capabilities. In DASH systems, the computation-intensive transcoding is the key technology to enable video rate adaptation, and cloud has become a preferred solution for massive video transcoding. Yet the cloud-based solution has the following two drawbacks. First, a video stream now has multiple versions after transcoding, which increases the network traffic traversing the core network. Second, the transcoding strategy is normally fixed and thus is not flexible to adapt to the dynamic change of viewers. Considering that mobile users, who normally experience dynamic network conditions from time to time, have occupied a very large portion of the total users, adaptive wireless transcoding is of great importance. To this end, we propose an adaptive wireless video transcoding framework based on the emerging edge computing paradigm by deploying edge transcoding servers close to base stations. With this design, the core network only needs to send the source video stream to the edge transcoding server rather than one stream for each viewer, and thus the network traffic across the core network is significantly reduced. Meanwhile, our edge transcoding server cooperates with the base station to transcode videos at a finer granularity according to the obtained users' channel conditions, which smartly adjusts the transcoding strategy to tackle with time-varying wireless channels. In order to improve the bandwidth utilization, we also develop efficient bandwidth adjustment algorithms that adaptively allocate the spectrum resources to individual mobile users. We validate the effectiveness of our proposed edge computing based framework through extensive simulations, which confirm the superiority of our framework.},
   author = {Desheng Wang and Yanrong Peng and Xiaoqiang Ma and Wenting Ding and Hongbo Jiang and Fei Chen and Jiangchuan Liu},
   doi = {10.1109/TSC.2018.2828426},
   issn = {19391374},
   issue = {5},
   journal = {IEEE Transactions on Services Computing},
   keywords = {Wireless video transcoding,edge computing},
   month = {9},
   pages = {685-697},
   publisher = {Institute of Electrical and Electronics Engineers},
   title = {Adaptive Wireless Video Streaming Based on Edge Computing: Opportunities and Approaches},
   volume = {12},
   year = {2019},
}
